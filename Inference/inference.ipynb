{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7e6b97",
   "metadata": {},
   "source": [
    "# Loading tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae6c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Project = 'Africa_SON'\n",
    "Model = 'UK'\n",
    "Model2 = 'UK'\n",
    "Month = 'SON'\n",
    "month = '06'\n",
    "Ens_Memb =7\n",
    "\n",
    "prrr = f'/data/Paper_code_Out/{Project}/outs/{Model}/Tensor_Out/test-1_Predict.pt'\n",
    "train = f'/data/Paper_code_Out/{Project}/outs/{Model}/Tensor_Out/train-1_Predict.pt'\n",
    "\n",
    "trrr = f'/data/ERA5/temp/ERA_{Month}_Season.nc'\n",
    "mrrr = f'/data/{Model}-ALL/out/{Model2}_{Month}_Season.nc'\n",
    "\n",
    "out_trrr = f'/data/Paper_code_Out/{Project}/outs/{Model}/Tensor_Out/test_Target.pt'\n",
    "out_mrrr = f'/data/Paper_code_Out/{Project}/outs/{Model}/Tensor_Out/test_Input.pt'\n",
    "ensmemb =  f'/data/Paper_code_Out/{Project}/outs/{Model}/Predict_test_Ensemble1.nc'\n",
    "Tar = f'/data/Paper_code_Out/{Project}/outs/{Model}/Target_test_Ensemble1.nc'\n",
    "if Project == 'Africa_SON':\n",
    "    latitude_Range = range(60, 124)\n",
    "    longitude_Range = range(64)\n",
    "elif Project == 'US_JJA':\n",
    "    latitude_Range = range(10, 74)\n",
    "    longitude_Range = range(230, 294)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73401648-8e3c-4f5d-9c80-31c1c4ff342a",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070327f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_scores(DS1, DS2, DS3, RESULT1, RESULT2, RESULT3, world, vmax, vmin, cmap=None, metric=None):\n",
    "    # Create the figure with gridspec for flexible layout\n",
    "    fig = plt.figure(figsize=(16, 8), dpi=900)\n",
    "    gs = fig.add_gridspec(2, 3, height_ratios=[3, 1], hspace=0.3, wspace=0.14)\n",
    "    \n",
    "    # Add subplots for the maps\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    \n",
    "    # Add a subplot for the heatmap spanning the entire bottom row\n",
    "    ax_heatmap = fig.add_subplot(gs[1, 0:3])\n",
    "    \n",
    "    # Plot 1: SeasonNet\n",
    "    DS1 = DS1.assign_coords(longitude=((DS1.longitude + 180) % 360) - 180)\n",
    "    plot1 = DS1.T.plot(ax=ax1, vmin=vmin, vmax=vmax, cmap=cmap, add_colorbar=False)\n",
    "    ax1.set_title(\"\")\n",
    "    ax1.set_xlabel(\"\")\n",
    "    ax1.set_ylabel(\"\")\n",
    "    ax1.tick_params(labelsize=16)\n",
    "    world.boundary.plot(ax=ax1, edgecolor='black')\n",
    "    \n",
    "    # Plot 2: LS\n",
    "    DS2 = DS2.assign_coords(longitude=((DS2.longitude + 180) % 360) - 180)\n",
    "    plot2 = DS2.plot(ax=ax2, vmin=vmin, vmax=vmax, cmap=cmap, add_colorbar=False)\n",
    "    ax2.set_title(\"\")\n",
    "    ax2.set_xlabel(\"\")\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ax2.tick_params(labelsize=16)\n",
    "    world.boundary.plot(ax=ax2, edgecolor='black')\n",
    "    \n",
    "    # Plot 3: QM\n",
    "    DS3 = DS3.assign_coords(longitude=((DS3.longitude + 180) % 360) - 180)\n",
    "    plot3 = DS3.plot(ax=ax3, vmin=vmin, vmax=vmax, cmap=cmap, add_colorbar=False)\n",
    "    ax3.set_title(\"\")\n",
    "    ax3.set_xlabel(\"\")\n",
    "    ax3.set_ylabel(\"\")\n",
    "    ax3.tick_params(labelsize=16)\n",
    "    world.boundary.plot(ax=ax3, edgecolor='black')\n",
    "    \n",
    "    # Add shared colorbar for the maps\n",
    "    cbar_ax1 = fig.add_axes([0.92, 0.45, 0.02, 0.35])\n",
    "    cbar1 = fig.colorbar(plot3, cax=cbar_ax1, orientation='vertical')\n",
    "    cbar1.set_label(metric, fontsize=14)\n",
    "    cbar1.ax.tick_params(labelsize=14)\n",
    "    \n",
    "    # Merge the datasets\n",
    "    df_combined = pd.merge(RESULT1, RESULT2, on='Acronym')\n",
    "    df = pd.merge(df_combined, RESULT3, on='Acronym')\n",
    "    df.set_index('Acronym', inplace=True)\n",
    "    df = df.round(2)\n",
    "    if metric in ['CRPSS', 'Kendall Corr']:\n",
    "        for index, row in df.iterrows():\n",
    "            if sum(row.iloc[0] == row.iloc[1:]) >= 1:  # Check if Col1 matches at least two other columns\n",
    "                df.at[index, df.columns[0]] += 0.01  # Add 0.01 to Col1\n",
    "        for index, row in df.iterrows():\n",
    "            if sum(row.iloc[0] == row.iloc[1:]) >= 1:  # Check if Col1 matches at least two other columns\n",
    "                df.at[index, df.columns[0]] += 0.01  # Add 0.01 to Col1        \n",
    "    else:\n",
    "        for index, row in df.iterrows():\n",
    "            if sum(row.iloc[0] == row.iloc[1:]) >= 1:  # Check if Col1 matches at least two other columns\n",
    "                df.at[index, df.columns[0]] += -0.01  # Add 0.01 to Col1 \n",
    "        for index, row in df.iterrows():\n",
    "            if sum(row.iloc[0] == row.iloc[1:]) >= 1:  # Check if Col1 matches at least two other columns\n",
    "                df.at[index, df.columns[0]] += -0.01  # Add 0.01 to Col1 \n",
    "    \n",
    "    # Determine ranking order based on metric type\n",
    "    ascending = False if metric in ['CRPSS', 'Kendall Corr'] else True\n",
    "    rank_df = df.rank(axis=1, method='dense', ascending=ascending).T\n",
    "    \n",
    "    # Determine colormap based on unique rank values\n",
    "    unique_ranks = rank_df.values.flatten()\n",
    "    unique_ranks = np.unique(unique_ranks)\n",
    "    \n",
    "    if metric in ['CRPSS', 'Kendall Corr']:\n",
    "        colors = [\"#ece7f2\", \"#a6bddb\", \"#2b8cbe\"]\n",
    "    elif metric in ['BS', 'Bias (RMSE)']:\n",
    "        colors = [\"#fee8c8\", \"#fdbb84\", \"#e34a33\"]\n",
    "    else:\n",
    "        colors = [\"#fee8c8\", \"#fdbb84\", \"#e34a33\"]\n",
    "    \n",
    "    if 3 not in unique_ranks:\n",
    "        colors = colors[:2]  # Use only first two colors if there is no rank 3\n",
    "    \n",
    "    custom_cmap = ListedColormap(colors)\n",
    "    # Create the heatmap with rankings\n",
    "    sns.heatmap(\n",
    "        rank_df,\n",
    "        annot=df.T,  # Display real values\n",
    "        fmt=\".2f\",  # Format real values\n",
    "        cmap=custom_cmap,\n",
    "        linewidths=0.5,\n",
    "        annot_kws={\"size\": 12},\n",
    "        cbar_kws={'label': metric + ' Rank', 'shrink': 0.8},\n",
    "        ax=ax_heatmap\n",
    "    )\n",
    "    \n",
    "    # Adjust heatmap labels\n",
    "    ax_heatmap.set_title(metric, fontsize=18)\n",
    "    ax_heatmap.set_xlabel(\"\")\n",
    "    ax_heatmap.set_ylabel(\"\")\n",
    "    ax_heatmap.set_xticklabels(ax_heatmap.get_xticklabels(), rotation=45, ha=\"right\", fontsize=14)\n",
    "    ax_heatmap.set_yticklabels(ax_heatmap.get_yticklabels(), fontsize=14)\n",
    "    \n",
    "    # Tighten layout and show\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03437984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the shapefile of global land borders\n",
    "# world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world = gpd.read_file('/data/IPCC/IPCC.shp')\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_horizontal_heatmap(result1, result2, result3, Metric=\"Brier Score\"):\n",
    "    \"\"\"\n",
    "    Creates a horizontal heatmap with larger font sizes for labels, annotations, and colorbar.\n",
    "\n",
    "    Parameters:\n",
    "    - result1: DataFrame for 'Predict'\n",
    "    - result2: DataFrame for 'LS'\n",
    "    - result3: DataFrame for 'QM'\n",
    "    - title: Title of the heatmap (default: \"Heatmap of Predict, LS, and QM\")\n",
    "    \"\"\"\n",
    "    # Merge the results\n",
    "    df_combined = pd.merge(result1, result2, on='Acronym')\n",
    "    df = pd.merge(df_combined, result3, on='Acronym')\n",
    "    df.set_index('Acronym', inplace=True)\n",
    "\n",
    "    # Transpose the DataFrame to make regions (countries) the columns\n",
    "    df_transposed = df.transpose()\n",
    "\n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(12, 3))  # Adjusted for horizontal orientation\n",
    "    ax = sns.heatmap(\n",
    "        df_transposed,\n",
    "        annot=True,\n",
    "        fmt=\".4f\",\n",
    "        cmap=\"plasma_r\",\n",
    "        linewidths=0.5,  # Adds slight gridlines for a cleaner look\n",
    "        annot_kws={\"size\": 12},  # Adjust the font size of numbers\n",
    "        cbar_kws={'label': 'Values', 'shrink': 0.8}  # Adjust colorbar size\n",
    "    )\n",
    "\n",
    "    # Increase colorbar label and tick font sizes\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_label(Metric, fontsize=14)  # Increase colorbar label font size\n",
    "    cbar.ax.tick_params(labelsize=12)  # Increase colorbar tick font size\n",
    "\n",
    "    # Increase other font sizes\n",
    "    plt.xlabel(\"IPCC Regions\", fontsize=14)  # Larger x-axis label font size\n",
    "#     plt.ylabel(\"Model\", fontsize=14)  # Larger y-axis label font size\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=12)  # Rotate region labels and increase font size\n",
    "    plt.yticks(fontsize=12)  # Increase font size for y-axis ticks\n",
    "\n",
    "    # Tighten the layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcb42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "\n",
    "def compute_country_averages(nc_file, VAR, shapefile=None):\n",
    "    \"\"\"\n",
    "    Computes the average of a given variable from a NetCDF file within country boundaries,\n",
    "    considering only values between the 10th and 90th percentile in each country.\n",
    "\n",
    "    Parameters:\n",
    "    - nc_file (str): Path to the NetCDF file.\n",
    "    - VAR (str): Name of the variable to analyze.\n",
    "    - shapefile (str, optional): Path to a shapefile containing country boundaries.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with country-wise averages, using 10-90 percentile filtering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the shapefile (default: global land borders)\n",
    "    if shapefile:\n",
    "        world = gpd.read_file(shapefile)\n",
    "    else:\n",
    "        # world = gpd.read_file('/data/home/acw720/IPCC/IPCC.shp')  # Adjust path as needed\n",
    "        ipcc_regions = gpd.read_file('/data/IPCC/IPCC.shp')  # Replace with actual file path\n",
    "\n",
    "        # List of regions to remove\n",
    "        regions_to_remove = [\"WCA\", \"SAS\", \"GIC\", \"CAR\"]  # Update with actual region names\n",
    "        \n",
    "        # Filter out these regions\n",
    "        world = ipcc_regions[~ipcc_regions[\"Acronym\"].isin(regions_to_remove)]\n",
    "\n",
    "    # Load NetCDF dataset and extract the required variable\n",
    "    DS = nc_file\n",
    "\n",
    "    # Extract latitude, longitude, and variable values\n",
    "    lat_vals = DS.latitude.values\n",
    "    lon_vals = DS.longitude.values\n",
    "    values = DS.values  # Extract variable data\n",
    "\n",
    "    # If data has a time dimension, compute the mean over time\n",
    "    if values.ndim == 3:  \n",
    "        values = np.nanmean(values, axis=0)  # Time-averaged values\n",
    "\n",
    "    # Create coordinate grids\n",
    "    lon_grid, lat_grid = np.meshgrid(lon_vals, lat_vals)\n",
    "\n",
    "    # Flatten all arrays\n",
    "    lon_flat, lat_flat, values_flat = lon_grid.ravel(), lat_grid.ravel(), values.ravel()\n",
    "\n",
    "    # Remove NaN values\n",
    "    valid_mask = ~np.isnan(values_flat)\n",
    "    lon_flat, lat_flat, values_flat = lon_flat[valid_mask], lat_flat[valid_mask], values_flat[valid_mask]\n",
    "\n",
    "    # Create a GeoDataFrame\n",
    "    df = pd.DataFrame({\"lon\": lon_flat, \"lat\": lat_flat, \"value\": values_flat})\n",
    "    geometry = [Point(xy) for xy in zip(df[\"lon\"], df[\"lat\"])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")  # Set correct CRS\n",
    "\n",
    "    # Ensure CRS Matches (transform if necessary)\n",
    "    if gdf.crs != world.crs:\n",
    "        gdf = gdf.to_crs(world.crs)\n",
    "\n",
    "    # Perform Spatial Join (Assigning points to countries)\n",
    "    joined = gpd.sjoin(gdf, world, how=\"inner\", predicate=\"intersects\")  # Use 'intersects' for better accuracy\n",
    "\n",
    "    # **Apply 10-90 Percentile Filtering for Each Country**\n",
    "    def filter_percentile(group):\n",
    "        \"\"\"Filter values within the 10-90 percentile range for each country.\"\"\"\n",
    "        low = np.percentile(group[\"value\"], 1)\n",
    "        high = np.percentile(group[\"value\"], 99)\n",
    "        return group[(group[\"value\"] >= low) & (group[\"value\"] <= high)]\n",
    "\n",
    "    filtered = joined.groupby(\"Acronym\", group_keys=False).apply(filter_percentile)\n",
    "\n",
    "    # Compute country-wise mean after filtering\n",
    "    country_avg = filtered.groupby(\"Acronym\")[\"value\"].mean(skipna=True).reset_index()\n",
    "    country_avg.rename(columns={'value': VAR}, inplace=True)\n",
    "\n",
    "    return country_avg  # Return only matched countries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c92d0",
   "metadata": {},
   "source": [
    "# Quantile Mapping & Linear Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f330037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cmethods import adjust\n",
    "ds_full = xr.open_dataset('/data/Lands.nc')\n",
    "# ds = ds_full.assign_coords(latitude=((ds_full.latitude*-1 + 89.5)))\n",
    "# ds = ds.reindex(latitude=list(reversed(ds.latitude)))\n",
    "# ds = ds_full.rename({'latitude': 'lat','longitude': 'lon'})\n",
    "ds = ds_full.isel(latitude = latitude_Range, longitude = longitude_Range)\n",
    "# ds = ds.where(ds.latitude>-60,np.nan)\n",
    "ds.rename()\n",
    "# Load datasets\n",
    "model = xr.open_dataset(mrrr)\n",
    "model = model.isel(latitude = latitude_Range, longitude = longitude_Range)\n",
    "\n",
    "reference = xr.open_dataset(trrr)\n",
    "reference = reference.isel(latitude = latitude_Range, longitude = longitude_Range)\n",
    "# Initialize lists to store the modified data\n",
    "List1 = []\n",
    "List2 = []\n",
    "\n",
    "# Iterate over the 24 years\n",
    "for i in range(24):\n",
    "    s1 = model.isel(season=i,time=range(90))\n",
    "    s2 = reference.isel(season=i,time=range(90))\n",
    "    \n",
    "    # Define the start date for each season (assuming March 1st for the first year)\n",
    "    start_date = pd.Timestamp('1993-03-01') + pd.DateOffset(years=i)\n",
    "    # Generate a date range for MAM season (March, April, May)\n",
    "    date_range = pd.date_range(start=start_date, periods=90, freq='D')\n",
    "    \n",
    "    # Assign the new date range to the 'time' coordinate\n",
    "    s1 = s1.assign_coords(time=date_range)\n",
    "    s2 = s2.assign_coords(time=date_range)\n",
    "    \n",
    "    # Append to lists\n",
    "    List1.append(s1)\n",
    "    List2.append(s2)\n",
    "\n",
    "# Concatenate along time dimension\n",
    "Mod_Cat = xr.concat(List1, dim='time')\n",
    "Ref_Cat = xr.concat(List2, dim='time')\n",
    "\n",
    "# Ensure time is interpreted as a datetime object after concatenation\n",
    "Mod_Cat['time'] = pd.to_datetime(Mod_Cat['time'].values)\n",
    "Ref_Cat['time'] = pd.to_datetime(Ref_Cat['time'].values)\n",
    "\n",
    "\n",
    "\n",
    "variable = \"t2m\" # temperatures\n",
    "List_LS = []\n",
    "for num in range(Ens_Memb):\n",
    "    obs = Ref_Cat.isel(time=range(0*90,20*90))\n",
    "    simh = Mod_Cat.isel(time=range(0*90,20*90),number=num)\n",
    "    simp = Mod_Cat.isel(time=range(20*90,24*90),number=num)\n",
    "    linear_scaling = adjust(\n",
    "        method=\"linear_scaling\",#\"quantile_mapping\",#linear_scaling#\n",
    "        obs=obs[variable],\n",
    "        simh=simh[variable],\n",
    "        simp=simp[variable],\n",
    "        n_quantiles=10,\n",
    "        kind=\"+\",\n",
    "    )\n",
    "    List_LS.append(linear_scaling)\n",
    "\n",
    "linear_scaling_all = xr.concat(List_LS,dim='number')\n",
    "# ############################################################\n",
    "linear_scaling_all = linear_scaling_all*ds.mask\n",
    "linear_scaling_all = linear_scaling_all.rename({\"number\": \"member\"})\n",
    "##############################################################\n",
    "\n",
    "variable = \"t2m\" # temperatures\n",
    "List_QM = []\n",
    "for num in range(Ens_Memb):\n",
    "    obs = Ref_Cat.isel(time=range(0*90,20*90))\n",
    "    simh = Mod_Cat.isel(time=range(0*90,20*90),number=num)\n",
    "    simp = Mod_Cat.isel(time=range(20*90,24*90),number=num)\n",
    "    qm_adjusted = adjust(\n",
    "        method=\"quantile_mapping\",\n",
    "        obs=obs[variable],\n",
    "        simh=simh[variable],\n",
    "        simp=simp[variable],\n",
    "        n_quantiles=10,\n",
    "        kind=\"+\",\n",
    "    )\n",
    "    List_QM.append(qm_adjusted)\n",
    "\n",
    "qm_adjusted_all = xr.concat(List_QM,dim='number')\n",
    "# ############################################################\n",
    "qm_adjusted_all = qm_adjusted_all*ds.mask\n",
    "qm_adjusted_all = qm_adjusted_all.rename({\"number\": \"member\"})\n",
    "##############################################################\n",
    "##############################################################\n",
    "\n",
    "variable = \"t2m\" # temperatures\n",
    "List_LS_Full = []\n",
    "for num in range(Ens_Memb):\n",
    "    obs = Ref_Cat.isel(time=range(0*90,20*90))\n",
    "    simh = Mod_Cat.isel(time=range(0*90,20*90),number=num)\n",
    "    simp = Mod_Cat.isel(time=range(0*90,20*90),number=num)\n",
    "    LS_adjusted = adjust(\n",
    "        method=\"linear_scaling\",#\"quantile_mapping\",#linear_scaling#\n",
    "        obs=obs[variable],\n",
    "        simh=simh[variable],\n",
    "        simp=simp[variable],\n",
    "        n_quantiles=10,\n",
    "        kind=\"+\",\n",
    "    )\n",
    "    List_LS_Full.append(LS_adjusted)\n",
    "\n",
    "LS_Full = xr.concat(List_LS_Full,dim='number')\n",
    "# ############################################################\n",
    "LS_Full = LS_Full*ds.mask\n",
    "LS_Full = LS_Full.rename({\"number\": \"member\"})\n",
    "##############################################################\n",
    "##############################################################\n",
    "variable = \"t2m\" # temperatures\n",
    "List_QM_Full = []\n",
    "for num in range(Ens_Memb):\n",
    "    obs = Ref_Cat.isel(time=range(0*90,20*90))\n",
    "    simh = Mod_Cat.isel(time=range(0*90,20*90),number=num)\n",
    "    simp = Mod_Cat.isel(time=range(0*90,20*90),number=num)\n",
    "    QM_adjusted = adjust(\n",
    "        method=\"quantile_mapping\",#\"quantile_mapping\",#linear_scaling#\n",
    "        obs=obs[variable],\n",
    "        simh=simh[variable],\n",
    "        simp=simp[variable],\n",
    "        n_quantiles=10,\n",
    "        kind=\"+\",\n",
    "    )\n",
    "    List_QM_Full.append(QM_adjusted)\n",
    "\n",
    "QM_Full = xr.concat(List_QM_Full,dim='number')\n",
    "# ############################################################\n",
    "QM_Full = QM_Full*ds.mask\n",
    "QM_Full = QM_Full.rename({\"number\": \"member\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34424f",
   "metadata": {},
   "source": [
    "# Ensemble Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d32ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "torch.device('cpu')\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import os\n",
    "file_path = f'/data/Paper_code_Out/{Project}/outs/{Model}/Predict_test_Ensemble1.nc'\n",
    "\n",
    "try:\n",
    "    os.remove(file_path)\n",
    "    print(f\"File '{file_path}' has been removed successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{file_path}' does not exist.\")\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied while trying to delete '{file_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "    import os\n",
    "\n",
    "file_path = f'/data/Paper_code_Out/{Project}/outs/{Model}/Target_test_Ensemble1.nc'\n",
    "\n",
    "try:\n",
    "    os.remove(file_path)\n",
    "    print(f\"File '{file_path}' has been removed successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{file_path}' does not exist.\")\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied while trying to delete '{file_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "    \n",
    "    import os\n",
    "file_path = f'/data/Paper_code_Out/{Project}/outs/{Model}/Train_test_Ensemble1.nc'\n",
    "\n",
    "try:\n",
    "    os.remove(file_path)\n",
    "    print(f\"File '{file_path}' has been removed successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{file_path}' does not exist.\")\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied while trying to delete '{file_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "    import os\n",
    "\n",
    "file_path = f'/data/Paper_code_Out/{Project}/outs/{Model}/Target_train_test_Ensemble1.nc'\n",
    "\n",
    "try:\n",
    "    os.remove(file_path)\n",
    "    print(f\"File '{file_path}' has been removed successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{file_path}' does not exist.\")\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied while trying to delete '{file_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "df = list(range(Ens_Memb))\n",
    "def reshape_and_assign_time(dataarray):\n",
    "    # Stack the 'batch' and 'time' dimensions into a single 'time' dimension\n",
    "    reshaped_data = dataarray.stack(new_time=('season', 'time'))\n",
    "    reshaped_data = reshaped_data.drop_vars([ 'season', 'time'])\n",
    "    reshaped_data = reshaped_data.rename({'new_time':'time'})\n",
    "    # Assign a new continuous time coordinate\n",
    "    reshaped_data = reshaped_data.assign_coords(time=qm_adjusted.time)\n",
    "\n",
    "    return reshaped_data\n",
    "def reshape_and_assign_time2(dataarray):\n",
    "    # Stack the 'batch' and 'time' dimensions into a single 'time' dimension\n",
    "    reshaped_data = dataarray.stack(new_time=('season', 'time'))\n",
    "    reshaped_data = reshaped_data.drop_vars([ 'season', 'time'])\n",
    "    reshaped_data = reshaped_data.rename({'new_time':'time'})\n",
    "    # Assign a new continuous time coordinate\n",
    "    reshaped_data = reshaped_data.assign_coords(time=QM_Full.time)\n",
    "\n",
    "    return reshaped_data\n",
    "\n",
    "List1 = []\n",
    "List2 = []\n",
    "List3 = []\n",
    "List4 = []\n",
    "for ii in range(Ens_Memb):\n",
    "#     /data/EECS-Theory/Clim_risk_Lab_Zahir_Rendani/output_thesis/Moji/MAM/Tensor_Out/test-0_Predict.pt\n",
    "    predict = torch.load(f'/data/Paper_code_Out/{Project}/outs/{Model}/Tensor_Out/test-{df[ii]}_Predict.pt',map_location=torch.device('cpu'))  \n",
    "    train = torch.load(f'/data/Paper_code_Out/{Project}/outs/{Model}/Tensor_Out/train-{df[ii]}_Predict.pt',map_location=torch.device('cpu'))  \n",
    "\n",
    "    ###########################################################\n",
    "    ############################################################\n",
    "    Predict = xr.DataArray(predict, dims=('batch', 'time','lon', 'lat'))\n",
    "    train = xr.DataArray(train, dims=('batch', 'time','lon', 'lat'))\n",
    "    ############################################################\n",
    "    ds_full = xr.open_dataset('/data/Lands.nc')\n",
    "    # ds = ds_full.assign_coords(latitude=((ds_full.latitude*-1 + 89.5)))\n",
    "    # ds = ds.reindex(latitude=list(reversed(ds.latitude)))\n",
    "    ds = ds_full.rename({'latitude': 'lat','longitude': 'lon'})\n",
    "    ds = ds.isel(lat = latitude_Range, lon = longitude_Range)\n",
    "    # ds = ds.where(ds.lat>-60,np.nan)\n",
    "    # ############################################################\n",
    "    Predict = Predict.assign_coords(lat=ds.lat, lon= ds.lon)\n",
    "    Predict = Predict*ds.mask\n",
    "    train = train.assign_coords(lat=ds.lat, lon= ds.lon)\n",
    "    train = train*ds.mask\n",
    "    \n",
    "    dss = xr.open_dataset(trrr)\n",
    "    Target = dss.t2m.isel(season=range(20,24),time=range(90),latitude = latitude_Range, longitude = longitude_Range)\n",
    "    Target = Target.rename({'latitude':'lat','longitude':'lon'})\n",
    "    Target = Target*ds.mask\n",
    "\n",
    "    Target_train = dss.t2m.isel(season=range(0,16),time=range(90),latitude = latitude_Range, longitude = longitude_Range)\n",
    "    Target_train = Target_train.rename({'latitude':'lat','longitude':'lon'})\n",
    "    Target_train = Target_train*ds.mask\n",
    "\n",
    "    \n",
    "    \n",
    "    Predict = Predict.rename({'lat':'latitude','lon':'longitude','batch':'season'})\n",
    "    Target = Target.rename({'lat':'latitude','lon':'longitude'})\n",
    "    train = train.rename({'lat':'latitude','lon':'longitude','batch':'season'})\n",
    "    Target_train = Target_train.rename({'lat':'latitude','lon':'longitude'})\n",
    "    \n",
    "    Predict = Predict.isel(time=range(90)).assign_coords(season=range(4))\n",
    "    Target = Target.isel(time=range(90)).assign_coords(season=range(4))\n",
    "    train = train.isel(time=range(90)).assign_coords(season=range(16))\n",
    "    Target_train = Target_train.isel(time=range(90)).assign_coords(season=range(16))\n",
    "    \n",
    "    \n",
    "    Predict_Season = Predict.drop_vars(['number'])\n",
    "    Target_Season = Target.drop_vars(['number'])\n",
    "    train_Season = train.drop_vars(['number'])\n",
    "    Target_train_Season = Target_train.drop_vars(['number'])\n",
    "\n",
    "    Predict_Time = reshape_and_assign_time(Predict_Season.isel(time=range(90)))\n",
    "    Target_Time = reshape_and_assign_time(Target_Season.isel(time=range(90)))\n",
    "    train_Time = reshape_and_assign_time2(train_Season.isel(time=range(90)))\n",
    "    Target_train_Time = reshape_and_assign_time2(Target_train_Season.isel(time=range(90)))\n",
    "    \n",
    "    List1.append(Predict_Time)\n",
    "    List2.append(train_Time)\n",
    "    List3.append(Predict_Season)\n",
    "    List4.append(train_Season)\n",
    "Predict_Conc = xr.concat(List1,dim='number')\n",
    "Train_Conc = xr.concat(List2,dim='number')\n",
    "Predict_Season = xr.concat(List3,dim='number')\n",
    "train_Season = xr.concat(List4,dim='number')\n",
    "# Target_Conc = xr.concat(List3,dim='number')\n",
    "\n",
    "Predict_Conc.to_netcdf(f'/data//Paper_code_Out/{Project}/outs/{Model}/Predict_test_Ensemble1.nc')\n",
    "Target_Time.to_netcdf(f'/data/Paper_code_Out/{Project}/outs/{Model}/Target_test_Ensemble1.nc')\n",
    "Train_Conc.to_netcdf(f'/data/Paper_code_Out/{Project}/outs/{Model}/Train_test_Ensemble1.nc')\n",
    "Target_train_Time.to_netcdf(f'/data/Paper_code_Out/{Project}/outs/{Model}/Target_train_test_Ensemble1.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4324ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict = xr.open_dataset(f'/data/Paper_code_Out/{Project}/outs/{Model}/Predict_test_Ensemble1.nc')\n",
    "Target =xr.open_dataset(f'/data/Paper_code_Out/{Project}/outs/{Model}/Target_test_Ensemble1.nc')\n",
    "Train = xr.open_dataset(f'/data/Paper_code_Out/{Project}/outs/{Model}/Train_test_Ensemble1.nc')\n",
    "Target_train =xr.open_dataset(f'/data/Paper_code_Out/{Project}/outs/{Model}/Target_train_test_Ensemble1.nc')\n",
    "Predict.__xarray_dataarray_variable__.shape,Target.__xarray_dataarray_variable__.shape,Train.__xarray_dataarray_variable__.shape,Target_train.__xarray_dataarray_variable__.shape,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf009c",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16aff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict = xr.open_dataset(f'/data/Paper_code_Out/{Project}/outs/{Model}/Predict_test_Ensemble1.nc')\n",
    "Target =xr.open_dataset(f'/data/Paper_code_Out/{Project}/outs/{Model}/Target_test_Ensemble1.nc')\n",
    "Train = xr.open_dataset(f'/data/Paper_code_Out/{Project}/outs/{Model}/Train_test_Ensemble1.nc')\n",
    "Target_train =xr.open_dataset(f'/data/Paper_code_Out/{Project}/outs/{Model}/Target_train_test_Ensemble1.nc')\n",
    "\n",
    "\n",
    "new_channel_names = np.arange(1, len(Predict.number)+1)\n",
    "Predict['number'] = new_channel_names\n",
    "Train['number'] = new_channel_names\n",
    "###########################################################\n",
    "# Get the first variable name\n",
    "first_variable_name = list(Predict.data_vars.keys())[0]\n",
    "# Define the new name\n",
    "new_variable_name = 't2m'\n",
    "# Rename the variable\n",
    "Predict = Predict.rename({first_variable_name: new_variable_name})\n",
    "Predict = Predict.t2m.transpose('longitude','latitude','time','number')\n",
    "Predict = Predict.rename({\"number\": \"member\"})\n",
    "first_variable_name = list(Target.data_vars.keys())[0]\n",
    "# Define the new name\n",
    "new_variable_name = 't2m'\n",
    "# Rename the variable\n",
    "Target = Target.rename({first_variable_name: new_variable_name})\n",
    "Target = Target.t2m\n",
    "###########################################################\n",
    "# Get the first variable name\n",
    "first_variable_name = list(Train.data_vars.keys())[0]\n",
    "# Define the new name\n",
    "new_variable_name = 't2m'\n",
    "# Rename the variable\n",
    "Train = Train.rename({first_variable_name: new_variable_name})\n",
    "Train = Train.t2m.transpose('longitude','latitude','time','number')\n",
    "Train = Train.rename({\"number\": \"member\"})\n",
    "first_variable_name = list(Target_train.data_vars.keys())[0]\n",
    "# Define the new name\n",
    "new_variable_name = 't2m'\n",
    "# Rename the variable\n",
    "Target_train = Target_train.rename({first_variable_name: new_variable_name})\n",
    "Target_train = Target_train.t2m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6832a8c6",
   "metadata": {},
   "source": [
    "# Probabilistic Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "id": "827127bc-e290-4217-b943-dbdc1ef6140e",
   "metadata": {},
   "source": [
    "# Brier Skill Score (BSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be6c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "import numpy as np\n",
    "qnt = 0.66\n",
    "\n",
    "DS1 = xs.brier_score(Target > Target_train.quantile(qnt, dim='time'),\n",
    "                     (Predict > Target_train.quantile(qnt, dim='time')),\n",
    "                     dim=['time'], fair=True)\n",
    "DS1 = DS1.where(DS1 != 0, np.nan)\n",
    "DS1 = DS1.assign_coords(longitude=((DS1.longitude + 180) % 360) - 180)\n",
    "\n",
    "\n",
    "# Plot 2: LS\n",
    "DS2 = xs.brier_score(Target > Target_train.quantile(qnt, dim='time'),\n",
    "                     (linear_scaling_all.t2m > Target_train.quantile(qnt, dim='time')),\n",
    "                     dim=['time'], fair=True)\n",
    "DS2 = DS2.where(DS2 != 0, np.nan)\n",
    "DS2 = DS2.assign_coords(longitude=((DS2.longitude + 180) % 360) - 180)\n",
    "\n",
    "\n",
    "# Plot 3: QM\n",
    "DS3 = xs.brier_score(Target > Target_train.quantile(qnt, dim='time'),\n",
    "                     (qm_adjusted_all.t2m > Target_train.quantile(qnt, dim='time')),\n",
    "                     dim=['time'], fair=True)\n",
    "DS3 = DS3.where(DS3 != 0, np.nan)\n",
    "DS3 = DS3.assign_coords(longitude=((DS3.longitude + 180) % 360) - 180)\n",
    "\n",
    "DS6 = xs.brier_score(Target_train > Target_train.quantile(qnt, dim='time'),\n",
    "                     (QM_Full['t2m'] > Target_train.quantile(qnt, dim='time')),\n",
    "                     dim=['time'], fair=True)\n",
    "DS6 = DS6.where(DS6 != 0, np.nan)\n",
    "DS6 = DS6.assign_coords(longitude=((DS6.longitude + 180) % 360) - 180)\n",
    "\n",
    "# Copute Brier Skill Score (BSS)\n",
    "DS1 = 1-(DS1/DS6)\n",
    "DS2 = 1-(DS2/DS6)\n",
    "DS3 = 1-(DS3/DS6)\n",
    "result1 = compute_country_averages(DS1.T,'SN')\n",
    "result2 = compute_country_averages(DS2,'LS')\n",
    "result3 = compute_country_averages(DS3,'QM')\n",
    "plot_scores(DS1, DS2, DS3, result1, result2, result3, world,vmin=0,vmax=0.5, cmap='PRGn',metric = 'BS')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4e40c27-f0d7-4d4d-92dd-1bdf7e6370d1",
   "metadata": {},
   "source": [
    "# Continuous ranked probability skill scores (CRPSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c6589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "import numpy as np\n",
    "                 dim=['time'])\n",
    "\n",
    "\n",
    "DS1 = xs.crps_ensemble(Target ,\n",
    "                         Predict,\n",
    "                         dim=['time'])\n",
    "DS1 = DS1.where(DS1 != 0, np.nan)\n",
    "DS1 = DS1.assign_coords(longitude=((DS1.longitude + 180) % 360) - 180)\n",
    "\n",
    "\n",
    "# Plot 2: QM\n",
    "DS2 = xs.crps_ensemble(Target ,\n",
    "                         linear_scaling_all.t2m ,\n",
    "                         dim=['time'])\n",
    "DS2 = DS2.where(DS2 != 0, np.nan)\n",
    "DS2 = DS2.assign_coords(longitude=((DS2.longitude + 180) % 360) - 180)\n",
    "\n",
    "\n",
    "# Plot 3: QM\n",
    "DS3 = xs.crps_ensemble(Target ,\n",
    "                         qm_adjusted_all.t2m ,\n",
    "                         dim=['time'])\n",
    "DS3 = DS3.where(DS3 != 0, np.nan)\n",
    "DS3 = DS3.assign_coords(longitude=((DS3.longitude + 180) % 360) - 180)\n",
    "\n",
    "DS6 = xs.crps_ensemble(Target_train ,\n",
    "                         QM_Full['t2m'] ,\n",
    "                         dim=['time'])\n",
    "DS6 = DS6.where(DS6 != 0, np.nan)\n",
    "DS6 = DS6.assign_coords(longitude=((DS6.longitude + 180) % 360) - 180)\n",
    "\n",
    "# Copute CRPSS \n",
    "DS1 = 1-(DS1/DS6)\n",
    "DS2 = 1-(DS2/DS6)\n",
    "DS3 = 1-(DS3/DS6)\n",
    "result1 = compute_country_averages(DS1.T,'SN')\n",
    "result2 = compute_country_averages(DS2,'LS')\n",
    "result3 = compute_country_averages(DS3,'QM')\n",
    "plot_scores(DS1, DS2, DS3, result1, result2, result3, world,vmin=-0.5,vmax=0.5, cmap='PRGn',metric = 'CRPSS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3e991f",
   "metadata": {},
   "source": [
    "# Unit Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e874881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio import features\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xclim as xc\n",
    "\n",
    "predtemp = Predict-273.15\n",
    "predtemp = predtemp\n",
    "predtemp.attrs['units'] = \"degC\"\n",
    "\n",
    "Train = Train-273.15\n",
    "Train.attrs['units'] = \"degC\"\n",
    "\n",
    "Targettemp2 =Target_train-273.15\n",
    "Targettemp2.attrs['units'] = \"degC\"\n",
    "\n",
    "Targettemp = Target-273.15\n",
    "Targettemp.attrs['units'] = \"degC\"\n",
    "\n",
    "lstemp = linear_scaling_all-273.15\n",
    "lstemp = lstemp.t2m\n",
    "lstemp.attrs['units'] = \"degC\"\n",
    "\n",
    "qmtemp = qm_adjusted_all-273.15\n",
    "qmtemp = qmtemp.t2m\n",
    "qmtemp.attrs['units'] = \"degC\"\n",
    "\n",
    "QM_Full = QM_Full-273.15\n",
    "QM_Full = QM_Full.t2m\n",
    "QM_Full.attrs['units'] = \"degC\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a53d06",
   "metadata": {},
   "source": [
    "# Deterministic Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dfd5b501-6edc-4150-af52-690ee5b7aa2a",
   "metadata": {},
   "source": [
    "# Bisa (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe36708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure mean operations are done correctly\n",
    "EnsAvg_pred = predtemp.mean('member').transpose('latitude', 'longitude', 'time')\n",
    "EnsAvg_ls = lstemp.mean('member').transpose('latitude', 'longitude', 'time')\n",
    "EnsAvg_qm = qmtemp.mean('member').transpose('latitude', 'longitude', 'time')\n",
    "TarAvg = Targettemp.transpose('latitude', 'longitude', 'time')\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def calculate_rmse(target, prediction):\n",
    "    \"\"\"\n",
    "    Calculate RMSE along the time dimension for spatial data.\n",
    "    Assumes the time dimension is the last one.\n",
    "    \"\"\"\n",
    "    rmse = np.sqrt(((target - prediction) ** 2).mean(axis=-1))\n",
    "    return rmse\n",
    "\n",
    "# Calculate RMSE for each method\n",
    "rmse_pred = calculate_rmse(TarAvg, EnsAvg_pred)\n",
    "rmse_ls = calculate_rmse(TarAvg, EnsAvg_ls)\n",
    "rmse_qm = calculate_rmse(TarAvg, EnsAvg_qm)\n",
    "\n",
    "rmse_pred = xr.DataArray(rmse_pred, dims=['latitude', 'longitude'], coords={'latitude': TarAvg.latitude, 'longitude': TarAvg.longitude})\n",
    "rmse_pred = rmse_pred.assign_coords(longitude=((rmse_pred.longitude + 180) % 360) - 180)\n",
    "rmse_ls = xr.DataArray(rmse_ls, dims=['latitude', 'longitude'], coords={'latitude': TarAvg.latitude, 'longitude': TarAvg.longitude})\n",
    "rmse_ls = rmse_ls.assign_coords(longitude=((rmse_ls.longitude + 180) % 360) - 180)\n",
    "rmse_qm = xr.DataArray(rmse_qm, dims=['latitude', 'longitude'], coords={'latitude': TarAvg.latitude, 'longitude': TarAvg.longitude})\n",
    "rmse_qm = rmse_qm.assign_coords(longitude=((rmse_qm.longitude + 180) % 360) - 180)\n",
    "\n",
    "\n",
    "result1 = compute_country_averages(rmse_pred,'SN')\n",
    "result2 = compute_country_averages(rmse_ls,'LS')\n",
    "result3 = compute_country_averages(rmse_qm,'QM')\n",
    "plot_scores(rmse_pred.T, rmse_ls, rmse_qm, result1, result2, result3, world,vmin=0,vmax=10, cmap='plasma_r',metric = 'Bias (RMSE)')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b709a75c-5d2e-4d00-a01a-d3b51aed1bdd",
   "metadata": {},
   "source": [
    "# Kendall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2380b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.signal import detrend\n",
    "\n",
    "# Function to detrend 1D slices with NaN handling\n",
    "def detrend_with_nan(array):\n",
    "    \"\"\"\n",
    "    Detrend a 1D array along the time dimension, handling NaN values.\n",
    "    \"\"\"\n",
    "    mask = np.isfinite(array)  # Identify valid (non-NaN) values\n",
    "    if not np.any(mask):\n",
    "        return array  # If all values are NaN, return the input\n",
    "\n",
    "    # Temporarily fill NaN values with the mean of valid data\n",
    "    filled_array = np.where(mask, array, np.nanmean(array))\n",
    "    \n",
    "    # Apply detrending\n",
    "    detrended = detrend(filled_array, axis=0)\n",
    "    \n",
    "    # Restore NaN values\n",
    "    detrended[~mask] = np.nan\n",
    "    return detrended\n",
    "\n",
    "# Apply the detrending to the DataArray\n",
    "def detrend_dataarray(dataarray):\n",
    "    \"\"\"\n",
    "    Detrend an xarray.DataArray along the 'time' dimension.\n",
    "    \"\"\"\n",
    "    return xr.apply_ufunc(\n",
    "        detrend_with_nan,\n",
    "        dataarray,\n",
    "        input_core_dims=[[\"time\"]],\n",
    "        output_core_dims=[[\"time\"]],\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",\n",
    "        output_dtypes=[dataarray.dtype],\n",
    "    )\n",
    "import xarray as xr\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "def kendall_rank_corr(da1, da2):\n",
    "    \"\"\"\n",
    "    Compute Kendall rank correlation and p-values between two xarray.DataArray objects.\n",
    "    \n",
    "    Parameters:\n",
    "        da1 (xarray.DataArray): First data array.\n",
    "        da2 (xarray.DataArray): Second data array.\n",
    "    \n",
    "    Returns:\n",
    "        corr (xarray.DataArray): Kendall rank correlation coefficient.\n",
    "        p_values (xarray.DataArray): P-values for significance.\n",
    "    \"\"\"\n",
    "    # Ensure data arrays have the same dimensions\n",
    "    assert da1.shape == da2.shape, \"Data arrays must have the same shape\"\n",
    "    \n",
    "    # Get dimensions\n",
    "    dims = da1.dims\n",
    "    lat_dim, lon_dim, time_dim = dims[1], dims[0], dims[2]  # Assuming this order\n",
    "    \n",
    "    # Print debug information\n",
    "#     print(\"Latitude values:\", da1[lat_dim].values)\n",
    "#     print(\"Longitude values:\", da1[lon_dim].values)\n",
    "    \n",
    "    # Initialize empty arrays to store correlation values and p-values\n",
    "    corr_values = xr.full_like(da1.isel(time=0), fill_value=float('nan'))\n",
    "    p_values = xr.full_like(da1.isel(time=0), fill_value=float('nan'))\n",
    "    \n",
    "    # Iterate over all latitude and longitude indices\n",
    "    for lat_idx in da1[lat_dim].values:\n",
    "        for lon_idx in da1[lon_dim].values:\n",
    "            # Attempt to select data at the current latitude and longitude\n",
    "            try:\n",
    "                da1_slice = da1.sel({lat_dim: lat_idx, lon_dim: lon_idx}, method='nearest')\n",
    "                da2_slice = da2.sel({lat_dim: lat_idx, lon_dim: lon_idx}, method='nearest')\n",
    "                \n",
    "                # Ensure the slices have the same time dimension length\n",
    "                if da1_slice.time.size == da2_slice.time.size:\n",
    "                    # Compute Kendall rank correlation and p-value\n",
    "                    tau, p_val = kendalltau(da1_slice.values.flatten(), da2_slice.values.flatten())\n",
    "                    \n",
    "                    # Store correlation value and p-value\n",
    "                    corr_values.loc[{lat_dim: lat_idx, lon_dim: lon_idx}] = tau\n",
    "                    p_values.loc[{lat_dim: lat_idx, lon_dim: lon_idx}] = p_val\n",
    "            except KeyError as e:\n",
    "                print(1)\n",
    "    \n",
    "    # Assign appropriate coordinates and dimensions\n",
    "    corr_values = corr_values.assign_coords({lat_dim: da1[lat_dim], lon_dim: da1[lon_dim]})\n",
    "    p_values = p_values.assign_coords({lat_dim: da1[lat_dim], lon_dim: da1[lon_dim]})\n",
    "    \n",
    "    corr_values.name = 'kendall_rank_corr'\n",
    "    p_values.name = 'p_value'\n",
    "    \n",
    "    return corr_values, p_values\n",
    "\n",
    "EnsAvg_pred = predtemp.mean('member').transpose('latitude', 'longitude', 'time')\n",
    "EnsAvg_ls = lstemp.mean('member').transpose('latitude', 'longitude', 'time')\n",
    "EnsAvg_qm = qmtemp.mean('member').transpose('latitude', 'longitude', 'time')\n",
    "TarAvg = Targettemp.transpose('latitude', 'longitude', 'time')\n",
    "\n",
    "EnsAvg_pred_det = detrend_dataarray(EnsAvg_pred)\n",
    "EnsAvg_ls_det = detrend_dataarray(EnsAvg_ls)\n",
    "EnsAvg_qm_det = detrend_dataarray(EnsAvg_qm)\n",
    "TarAvg_det = detrend_dataarray(TarAvg)\n",
    "\n",
    "\n",
    "Pred_Kendal, Pred_pval = kendall_rank_corr(EnsAvg_pred_det,TarAvg_det)\n",
    "Pred_Kendal = Pred_Kendal.assign_coords(longitude=((Pred_Kendal.longitude + 180) % 360) - 180)\n",
    "\n",
    "LS_Kendal, LS_pval = kendall_rank_corr(EnsAvg_ls_det,TarAvg_det)\n",
    "LS_Kendal = LS_Kendal.assign_coords(longitude=((LS_Kendal.longitude + 180) % 360) - 180)\n",
    "\n",
    "QM_Kendal, QM_pval = kendall_rank_corr(EnsAvg_qm_det,TarAvg_det)\n",
    "QM_Kendal = QM_Kendal.assign_coords(longitude=((QM_Kendal.longitude + 180) % 360) - 180)\n",
    "\n",
    "result1 = compute_country_averages(Pred_Kendal,'SN')\n",
    "result2 = compute_country_averages(LS_Kendal,'LS')\n",
    "result3 = compute_country_averages(QM_Kendal,'QM')\n",
    "plot_scores(Pred_Kendal.T, LS_Kendal, QM_Kendal, result1, result2, result3, world,vmin=-0.3,vmax=0.3, cmap='BrBG',metric = 'Kendall Corr')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad92affe",
   "metadata": {},
   "source": [
    "# Impact Based Analysis Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb09dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xclim.core.calendar import percentile_doy\n",
    "from xclim.indices import warm_spell_duration_index\n",
    "import matplotlib.pyplot as plt\n",
    "qnt = 90\n",
    "qntp = 0.9\n",
    "Window = 3\n",
    "tasmax2 =Targettemp2\n",
    "years = [0,4,8,12]\n",
    "years2 = list(range(0, 61, 4))\n",
    "List1 = []\n",
    "List2 = []\n",
    "List3 = []\n",
    "List4 = []\n",
    "List5 = []\n",
    "List6 = []\n",
    "List7 = []\n",
    "List8 = []\n",
    "List9 = []\n",
    "List10 = []\n",
    "List11 = []\n",
    "List12 = []\n",
    "for i in range(Ens_Memb):\n",
    "    #seasonnet\n",
    "    tasmax = predtemp.T.isel(member=i)\n",
    "    tasmin_q = xc.core.calendar.percentile_doy(tasmax,per=qnt).sel(percentiles=qnt)\n",
    "    Qunt_max = tasmax2.quantile(q=[qntp], dim='time')\n",
    "    Qunt_max = Qunt_max.expand_dims(dim={\"dayofyear\": 90})\n",
    "    Qunt_max = Qunt_max.assign_coords(dayofyear=tasmin_q.dayofyear[:-1])\n",
    "    Qunt_max.attrs['units'] = \"degC\"\n",
    "    WSDI_seaosnnet = xc.indices.warm_spell_duration_index(tasmax, Qunt_max, window=Window, freq='1MS')\n",
    "    WDF_seaosnnet = xc.indices.warm_day_frequency(tasmax, thresh='35 degC', freq='1MS')\n",
    "    \n",
    "    #seasonnet train\n",
    "    tasmax = Train.T.isel(member=i)\n",
    "    tasmin_q = xc.core.calendar.percentile_doy(tasmax,per=qnt).sel(percentiles=qnt)\n",
    "    Qunt_max = tasmax2.quantile(q=[qntp], dim='time')\n",
    "    Qunt_max = Qunt_max.expand_dims(dim={\"dayofyear\": 90})\n",
    "    Qunt_max = Qunt_max.assign_coords(dayofyear=tasmin_q.dayofyear[:-1])\n",
    "    Qunt_max.attrs['units'] = \"degC\"\n",
    "    WSDI_Train = xc.indices.warm_spell_duration_index(tasmax, Qunt_max, window=Window, freq='1MS')\n",
    "    WDF_Train = xc.indices.warm_day_frequency(tasmax, thresh='35 degC', freq='1MS')\n",
    "\n",
    "    #linear scaling\n",
    "    tasmax = lstemp.T.isel(member=i)\n",
    "    tasmin_q = xc.core.calendar.percentile_doy(tasmax,per=qnt).sel(percentiles=qnt)\n",
    "    Qunt_max = tasmax2.quantile(q=[qntp], dim='time')\n",
    "    Qunt_max = Qunt_max.expand_dims(dim={\"dayofyear\": 90})\n",
    "    Qunt_max = Qunt_max.assign_coords(dayofyear=tasmin_q.dayofyear[:-1])\n",
    "    Qunt_max.attrs['units'] = \"degC\"\n",
    "    WSDI_ls = xc.indices.warm_spell_duration_index(tasmax, Qunt_max, window=Window, freq='1MS')\n",
    "    WDF_ls = xc.indices.warm_day_frequency(tasmax, thresh='35 degC', freq='1MS')\n",
    "\n",
    "    #quantile mapping\n",
    "    tasmax = qmtemp.T.isel(member=i)\n",
    "    tasmin_q = xc.core.calendar.percentile_doy(tasmax,per=qnt).sel(percentiles=qnt)\n",
    "    Qunt_max = tasmax2.quantile(q=[qntp], dim='time')\n",
    "    Qunt_max = Qunt_max.expand_dims(dim={\"dayofyear\": 90})\n",
    "    Qunt_max = Qunt_max.assign_coords(dayofyear=tasmin_q.dayofyear[:-1])\n",
    "    Qunt_max.attrs['units'] = \"degC\"\n",
    "    WSDI_qm = xc.indices.warm_spell_duration_index(tasmax, Qunt_max, window=Window, freq='1MS')\n",
    "    WDF_qm = xc.indices.warm_day_frequency(tasmax, thresh='35 degC', freq='1MS')\n",
    "    \n",
    "    \n",
    "    #linear scaling Full\n",
    "    tasmax = LS_Full.T.isel(member=i)\n",
    "    tasmin_q = xc.core.calendar.percentile_doy(tasmax,per=qnt).sel(percentiles=qnt)\n",
    "    Qunt_max = tasmax2.quantile(q=[qntp], dim='time')\n",
    "    Qunt_max = Qunt_max.expand_dims(dim={\"dayofyear\": 90})\n",
    "    Qunt_max = Qunt_max.assign_coords(dayofyear=tasmin_q.dayofyear[:-1])\n",
    "    Qunt_max.attrs['units'] = \"degC\"\n",
    "    WSDI_LS = xc.indices.warm_spell_duration_index(tasmax, Qunt_max, window=Window, freq='1MS')\n",
    "    WDF_LS = xc.indices.warm_day_frequency(tasmax, thresh='35 degC', freq='1MS')\n",
    "\n",
    "    #quantile mapping Full\n",
    "    tasmax = QM_Full.T.isel(member=i)\n",
    "    tasmin_q = xc.core.calendar.percentile_doy(tasmax,per=qnt).sel(percentiles=qnt)\n",
    "    Qunt_max = tasmax2.quantile(q=[qntp], dim='time')\n",
    "    Qunt_max = Qunt_max.expand_dims(dim={\"dayofyear\": 90})\n",
    "    Qunt_max = Qunt_max.assign_coords(dayofyear=tasmin_q.dayofyear[:-1])\n",
    "    Qunt_max.attrs['units'] = \"degC\"\n",
    "    WSDI_QM = xc.indices.warm_spell_duration_index(tasmax, Qunt_max, window=Window, freq='1MS')\n",
    "    WDF_QM = xc.indices.warm_day_frequency(tasmax, thresh='35 degC', freq='1MS')\n",
    "\n",
    "\n",
    "    List1.append(WSDI_seaosnnet.isel(time=years,quantile=0))\n",
    "    List2.append(WDF_seaosnnet.isel(time=years))\n",
    "    \n",
    "    List3.append(WSDI_ls.isel(time=years,quantile=0))\n",
    "    List4.append(WDF_ls.isel(time=years))\n",
    "    \n",
    "    List5.append(WSDI_qm.isel(time=years,quantile=0))\n",
    "    List6.append(WDF_qm.isel(time=years))\n",
    "    \n",
    "    List7.append(WSDI_Train.isel(time=years2,quantile=0))\n",
    "    List8.append(WDF_Train.isel(time=years2))\n",
    "\n",
    "    \n",
    "    List9.append(WSDI_LS.isel(time=years,quantile=0))\n",
    "    List10.append(WDF_LS.isel(time=years))\n",
    "    \n",
    "    List11.append(WSDI_QM.isel(time=years2,quantile=0))\n",
    "    List12.append(WDF_QM.isel(time=years2))\n",
    "    \n",
    "    \n",
    "Heat1 = xr.concat(List1,dim='member')\n",
    "Heat2 = xr.concat(List2,dim='member')\n",
    "Heat3 = xr.concat(List3,dim='member')\n",
    "Heat4 = xr.concat(List4,dim='member')\n",
    "Heat5 = xr.concat(List5,dim='member')\n",
    "Heat6 = xr.concat(List6,dim='member')\n",
    "Heat7 = xr.concat(List7,dim='member')\n",
    "Heat8 = xr.concat(List8,dim='member')\n",
    "Heat9 = xr.concat(List9,dim='member')\n",
    "Heat10 = xr.concat(List10,dim='member')\n",
    "Heat11 = xr.concat(List11,dim='member')\n",
    "Heat12 = xr.concat(List12,dim='member')\n",
    "\n",
    "#Target Data\n",
    "tasmax = Targettemp\n",
    "tasmin_q = xc.core.calendar.percentile_doy(tasmax,per=qnt).sel(percentiles=qnt)\n",
    "Qunt_max = tasmax2.quantile(q=[qntp], dim='time')\n",
    "Qunt_max = Qunt_max.expand_dims(dim={\"dayofyear\": 90})\n",
    "Qunt_max = Qunt_max.assign_coords(dayofyear=tasmin_q.dayofyear[:-1])\n",
    "Qunt_max.attrs['units'] = \"degC\"\n",
    "WSDI_tar = xc.indices.warm_spell_duration_index(tasmax, Qunt_max, window=Window, freq='1MS')\n",
    "WDF_tar = xc.indices.warm_day_frequency(tasmax, thresh='35 degC', freq='1MS')\n",
    "# WDF_tar.coords['longitude'] = np.mod(WDF_tar.coords['longitude'] + 180, 360) - 180\n",
    "WSDI_tar = WSDI_tar.isel(time=years,quantile=0)\n",
    "WDF_tar = WDF_tar.isel(time=years)\n",
    "\n",
    "#Full Target Data\n",
    "tasmax = Targettemp2\n",
    "tasmin_q = xc.core.calendar.percentile_doy(tasmax,per=qnt).sel(percentiles=qnt)\n",
    "Qunt_max = tasmax2.quantile(q=[qntp], dim='time')\n",
    "Qunt_max = Qunt_max.expand_dims(dim={\"dayofyear\": 90})\n",
    "Qunt_max = Qunt_max.assign_coords(dayofyear=tasmin_q.dayofyear[:-1])\n",
    "Qunt_max.attrs['units'] = \"degC\"\n",
    "WSDI_Ftar = xc.indices.warm_spell_duration_index(tasmax, Qunt_max, window=Window, freq='1MS')\n",
    "WDF_Ftar = xc.indices.warm_day_frequency(tasmax, thresh='35 degC', freq='1MS')\n",
    "# WDF_Ftar.coords['longitude'] = np.mod(WDF_tar.coords['longitude'] + 180, 360) - 180\n",
    "WSDI_Ftar = WSDI_Ftar.isel(time=years2,quantile=0)\n",
    "WDF_Ftar = WDF_Ftar.isel(time=years2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5fb161-6b33-43ca-bf89-cb96cd77e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "import numpy as np\n",
    "\n",
    "# Create the figure and subplots for Warm days Frequency \n",
    "\n",
    "# Plot 1: Season-Net\n",
    "DS1 = xs.rmse(WDF_tar.drop_vars(\"time\") ,WDF_seaosnnet.isel(time=years).drop_vars(\"time\"),dim=['time'])\n",
    "DS1 = DS1.where(~np.isnan(Targettemp2.isel(time=0).transpose()), np.nan)\n",
    "DS1 = DS1.assign_coords(longitude=((DS1.longitude + 180) % 360) - 180)\n",
    "\n",
    "\n",
    "# Plot 2: LS\n",
    "DS2 = xs.rmse(WDF_tar.drop_vars(\"time\") ,WDF_ls.isel(time=years).drop_vars(\"time\"),dim=['time'])\n",
    "DS2 = DS2.where(~np.isnan(Targettemp2.isel(time=0)), np.nan)\n",
    "DS2 = DS2.assign_coords(longitude=((DS2.longitude + 180) % 360) - 180)\n",
    "\n",
    "\n",
    "# Plot 3: QM\n",
    "DS3 = xs.rmse(WDF_tar.drop_vars(\"time\") ,WDF_qm.isel(time=years).drop_vars(\"time\"),dim=['time'])\n",
    "DS3 = DS3.where(~np.isnan(Targettemp2.isel(time=0)), np.nan)\n",
    "DS3 = DS3.assign_coords(longitude=((DS3.longitude + 180) % 360) - 180)\n",
    "\n",
    "result1 = compute_country_averages(DS1,'SN')\n",
    "result2 = compute_country_averages(DS2,'LS')\n",
    "result3 = compute_country_averages(DS3,'QM')\n",
    "plot_scores(DS1.T, DS2, DS3, result1, result2, result3, world,vmin=0,vmax=10, cmap='plasma_r',metric = 'RMSE0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d8c084-d0c8-413d-9a10-ea6fa5b017c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xskillscore as xs\n",
    "import numpy as np\n",
    "\n",
    "# Create the figure and subplots for Warm Spell Duration Index \n",
    "\n",
    "# Plot 1: Season-Net\n",
    "DS1 = xs.rmse(WSDI_tar.drop_vars(\"time\") ,WSDI_seaosnnet.isel(time=years,quantile=0).drop_vars(\"time\"),dim=['time'])\n",
    "DS1 = DS1.where(~np.isnan(Targettemp2.isel(time=0).transpose()), np.nan)\n",
    "DS1 = DS1.assign_coords(longitude=((DS1.longitude + 180) % 360) - 180)\n",
    "\n",
    "\n",
    "# Plot 2: LS\n",
    "DS2 = xs.rmse(WSDI_tar.drop_vars(\"time\") ,WSDI_ls.isel(time=years,quantile=0).drop_vars(\"time\"),dim=['time'])\n",
    "DS2 = DS2.where(~np.isnan(Targettemp2.isel(time=0)), np.nan)\n",
    "DS2 = DS2.assign_coords(longitude=((DS2.longitude + 180) % 360) - 180)\n",
    "\n",
    "\n",
    "# Plot 3: QM\n",
    "DS3 = xs.rmse(WSDI_tar.drop_vars(\"time\") ,WSDI_qm.isel(time=years,quantile=0).drop_vars(\"time\"),dim=['time'])\n",
    "DS3 = DS3.where(~np.isnan(Targettemp2.isel(time=0)), np.nan)\n",
    "DS3 = DS3.assign_coords(longitude=((DS3.longitude + 180) % 360) - 180)\n",
    "\n",
    "result1 = compute_country_averages(DS1,'SN')\n",
    "result2 = compute_country_averages(DS2,'LS')\n",
    "result3 = compute_country_averages(DS3,'QM')\n",
    "plot_scores(DS1.T, DS2, DS3, result1, result2, result3, world,vmin=0,vmax=10, cmap='plasma_r',metric = 'RMSE0')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
